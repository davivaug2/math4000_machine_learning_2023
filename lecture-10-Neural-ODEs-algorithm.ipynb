{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural ODEs algorithm\n",
    "\n",
    "In this notebook, we will derive the adjoint equation for neural ODEs using Lagrange multipliers. We will then use this adjoint equation to derive the gradient of the loss function with respect to the parameters of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimization problem\n",
    "\n",
    "Here is our minimization problem from neural ODEs:\n",
    "\n",
    "$$\n",
    "\\textrm{argmin}_{\\theta} L(z(t_1))\n",
    "$$\n",
    "\n",
    "subject to\n",
    "$$\n",
    "\\frac{dz}{dt} = f(z(t), t, \\theta), \\quad z(t_0) = z_0, \\quad t_0 < t_1\n",
    "$$\n",
    "\n",
    "- $f$ is the neural network with parameters $\\theta$\n",
    "- $z(t_0)$ is the input, $z(t_1)$ is the output, $z(t)$ is the state reached from $z(t_0)$ at time $t\\in[t_0, t_1]$\n",
    "- $L$ is the loss function and it is a function of the output $z(t_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: \n",
    "\n",
    "The minimization problem: find out $\\theta$ such that\n",
    "$$\n",
    "L(z(1)) = (z(1) - 1)^2\n",
    "$$\n",
    "reaches the minimum value where $z(t)$\n",
    "subject to\n",
    "$$\n",
    "\\frac{dz}{dt} = -z(t) + \\theta, \\quad z(0) = 2, \\quad 0< t < 1\n",
    "$$\n",
    "\n",
    "#### Solution:\n",
    "We first solve the ODE using the integrating factor method:\n",
    "$$\n",
    "(e^{t} z(t))' = e^{t} (z'(t) + z(t)) = e^{t} \\theta\n",
    "$$\n",
    "Then integrating the above equation, we get:\n",
    "$$\n",
    "e^{t} z(t) - e^{0} z(0) = \\int_{0}^{t} e^{t} \\theta dt = \\theta \\left( e^{t} - 1 \\right)\n",
    "$$\n",
    "Using the initial condition $z(0) = 2$, we get:\n",
    "$$\n",
    "z(t) = \\theta + (2 - \\theta) e^{-t}\n",
    "$$\n",
    "Then we compute the loss:\n",
    "$$\n",
    "L(z(1)) = (z(1) - 1)^2 = (\\theta + (2 - \\theta) e^{-1} - 1)^2 = (\\theta(1 - e^{-1}) + 2e^{-1} - 1)^2\n",
    "$$\n",
    "Then this loss reaches the minimum by setting\n",
    "$$\n",
    "\\theta(1 - e^{-1}) + 2e^{-1} - 1 = 0\n",
    "$$\n",
    "We solve out\n",
    "$$\n",
    "\\theta = \\frac{1 - 2e^{-1}}{1 - e^{-1}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General case\n",
    "\n",
    "For general $f(z, t, \\theta)$ (represented by a neural network with parameter $\\theta$), we cannot solve the ODE analytically. This is a constrained nonlinear optimization problem. To solve it, we need to compute the gradient of the loss function $L(z(t_1))$ with respect to the neural network parameters $\\theta$:\n",
    "$$\n",
    "\\frac{\\partial L(z(t_1))}{\\partial \\theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of neural ODEs:\n",
    "\n",
    "The restriction of neural ODEs is that the input $z(t_0)$ and output $z(t_1)$ of the neural network must be the same dimension. However, we want the input and output of the neural network to be different dimensions. For example, we want to classify images. The input of the neural network is an image and the output of the neural network is the classification result.\n",
    "\n",
    "To solve this problem, we put two feedforward neural networks before the initial condition and after the output of the ODE. The initial condition is the output of the first neural network and the output of the ODE is the input of the second neural network. The output of the second neural network is the classification result.\n",
    "\n",
    "The function $y=f(x; \\theta, \\theta_1, \\theta_2)$ can be represented by:\n",
    "$$\n",
    "\\frac{dz}{dt} = f(z(t), t, \\theta),\n",
    "$$\n",
    "with the initial condition\n",
    "$$\n",
    "z(t_0) = g(x, \\theta_1)\n",
    "$$\n",
    "and the output determined by the solution\n",
    "$$\n",
    "y = h(z(t_1), \\theta_2)\n",
    "$$\n",
    "Here $g$ and $h$ are two feedforward neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Method for computing the gradient\n",
    "\n",
    "Let us take the gradient directly using the chain rule:\n",
    "$$\n",
    "\\frac{\\partial L(z(t_1))}{\\partial \\theta} = \\frac{\\partial L(z(t_1))}{\\partial z(t_1)} \\frac{\\partial z(t_1)}{\\partial \\theta}\n",
    "$$\n",
    "The first term $\\frac{\\partial L(z(t_1))}{\\partial z(t_1)}$ is easy to compute by just taking gradient of the loss function.\n",
    "\n",
    "Now we focus on the second $\\frac{\\partial z(t_1)}{\\partial \\theta}$. To compute $z(t_1)$, we integrate the ODE in time and obtain:\n",
    "$$\n",
    "z(t_1) = z(t_0) + \\int_{t_0}^{t_1} f(z(t), t, \\theta) dt\n",
    "$$\n",
    "Then we take the gradient with respect to $\\theta$. Notice that: \n",
    "- $z(t_0)$ is independent of $\\theta$\n",
    "- $z(t)$ is a function of $\\theta$, we need to use the chain rule:\n",
    "$$\n",
    "\\frac{\\partial z(t_1)}{\\partial \\theta} = \\int_{t_0}^{t_1} \\frac{\\partial }{\\partial \\theta} (f(z(t), t, \\theta)) dt = \\int_{t_0}^{t_1} \\frac{\\partial f(z(t), t, \\theta)}{\\partial z(t)} \\frac{\\partial z(t)}{\\partial \\theta} + \\frac{\\partial f(z(t), t, \\theta)}{\\partial \\theta}dt\n",
    "$$\n",
    "We do not have good idea of how to compute $\\frac{\\partial z(t)}{\\partial \\theta}$, since $z(t)$ depends implicitly on $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagrange Multipliers\n",
    "\n",
    "We can use Lagrange multipliers to solve this problem. We will first derive the adjoint equation for the ODE, then use the adjoint equation to derive the gradient of the loss function with respect to the neural network parameters.\n",
    "\n",
    "### Review of Lagrange Multipliers in calculus\n",
    "\n",
    "Let us review Lagrange multipliers in calculus. Suppose we have a constrained optimization problem:\n",
    "$$\n",
    "\\textrm{min}_{x} f(x)\n",
    "$$\n",
    "subject to\n",
    "$$\n",
    "g(x) = 0\n",
    "$$\n",
    "\n",
    "#### Example:\n",
    "$$\n",
    "\\textrm{min}_{x,y} (x^2 + y^2)\n",
    "$$\n",
    "subject to\n",
    "$$\n",
    "x + y = 0\n",
    "$$\n",
    "\n",
    "We can use Lagrange multipliers to solve this problem. We introduce a Lagrange multiplier $\\lambda$ and rewrite the constrained optimization problem as:\n",
    "$$\n",
    "\\textrm{argmin}_{x, \\lambda} L(x,\\lambda)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "L(x,\\lambda) = f(x) + \\lambda g(x)\n",
    "$$\n",
    "Then we take the gradient of the objective function with respect to $x$ and $\\lambda$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial x} &= 0 \\\\\n",
    "\\frac{\\partial L}{\\partial \\lambda} &= 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "or equivalently:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial f(x)}{\\partial x} + \\lambda \\frac{\\partial g(x)}{\\partial x} &= 0 \\\\\n",
    "g(x) &= 0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving the adjoint equation for neural ODEs using Lagrange multipliers\n",
    "\n",
    "We will now derive the adjoint equation for neural ODEs using Lagrange multipliers. We will first derive the adjoint equation for the ODE, then use the adjoint equation to derive the gradient of the loss function with respect to the neural network parameters.\n",
    "\n",
    "We define the Lagrangian function $\\psi = \\psi(\\theta)$:\n",
    "$$\n",
    "\\psi(\\theta) = L(z(t_1)) - \\int_{t_0}^{t_1} \\lambda(t) (\\frac{dz}{dt} - f(z(t), t, \\theta)) dt\n",
    "$$\n",
    "where $\\lambda(t)$ is the Lagrange multiplier and the function $z(t)$ satisfies the ODE:\n",
    "$$\n",
    "\\frac{dz}{dt} = f(z(t), t, \\theta), \\quad z(t_0) = z_0, \\quad t_0 < t_1\n",
    "$$\n",
    "\n",
    "Notice that $z(t)$ satisfies the ODE, so the second term in $\\psi(\\theta)$ is equal to zero. Then the second term does not contribute to the gradient of $\\psi(\\theta)$ with respect to $\\theta$, that is:\n",
    "$$\n",
    "\\frac{\\partial \\psi(\\theta)}{\\partial \\theta} = \\frac{\\partial L(z(t_1))}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "Now we just need to compute $\\frac{\\partial \\psi(\\theta)}{\\partial \\theta}$ and it is equal to $\\frac{\\partial L(z(t_1))}{\\partial \\theta}$.\n",
    "\n",
    "Why we want to introduce the Lagrange multiplier $\\lambda(t)$? We want to choose $\\lambda(t)$ such that we can hopefully eliminate the difficulty in computing $\\frac{\\partial z(t)}{\\partial \\theta}$. Now our problem is to find $\\lambda(t)$ such that the gradient of $\\psi(\\theta)$ with respect to $\\theta$ is easy to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplify terms\n",
    "\n",
    "Let us use integration by parts to compute\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\int_{t_0}^{t_1} \\lambda(t) (\\frac{dz}{dt} - f(z(t), t, \\theta)) dt \\\\\n",
    "&= \\int_{t_0}^{t_1} \\lambda(t) \\frac{dz}{dt} dt - \\int_{t_0}^{t_1} \\lambda(t) f(z(t), t, \\theta) dt \\\\\n",
    "&= \\lambda(t_1) z(t_1) - \\lambda(t_0) z(t_0) - \\int_{t_0}^{t_1}\\frac{d \\lambda(t)}{dt} z(t) - \\int_{t_0}^{t_1} \\lambda(t) f(z(t), t, \\theta) dt \\\\\n",
    "&= \\lambda(t_1) z(t_1) - \\lambda(t_0) z_0 - \\int_{t_0}^{t_1} (\\frac{d \\lambda(t)}{dt} z(t) + \\lambda(t) f(z(t), t, \\theta) ) dt\n",
    "\\end{aligned}\n",
    "$$\n",
    "Now we can use chain rule to compute the gradient with respect to $\\theta$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\frac{\\partial }{\\partial \\theta} (\\int_{t_0}^{t_1} \\lambda(t) (\\frac{dz}{dt} - f(z(t), t, \\theta)) dt) \\\\\n",
    "=& \\frac{\\partial }{\\partial \\theta} (\\lambda(t_1) z(t_1) - \\lambda(t_0) z_0 - \\int_{t_0}^{t_1} (\\frac{d \\lambda(t)}{dt} z(t) + \\lambda(t) f(z(t), t, \\theta) ) dt) \\\\\n",
    "=& \\lambda(t_1) \\frac{\\partial z(t_1)}{\\partial \\theta} - \\lambda(t_0) \\frac{\\partial z_0}{\\partial \\theta} \\\\\n",
    "&- \\int_{t_0}^{t_1} (\\frac{d \\lambda(t)}{dt} \\frac{\\partial z(t)}{\\partial \\theta} + \\lambda(t) ( \\frac{\\partial f(z(t), t, \\theta)}{\\partial z}\\frac{\\partial z}{\\partial \\theta} + \\frac{\\partial f(z(t), t, \\theta)}{\\partial \\theta} ) ) dt \\\\\n",
    "=& \\lambda(t_1) \\frac{\\partial z(t_1)}{\\partial \\theta} - \\int_{t_0}^{t_1} (\\frac{d \\lambda(t)}{dt} \\frac{\\partial z(t)}{\\partial \\theta} + \\lambda(t) ( \\frac{\\partial f(z(t), t, \\theta)}{\\partial z}\\frac{\\partial z}{\\partial \\theta} + \\frac{\\partial f(z(t), t, \\theta)}{\\partial \\theta} ) ) dt\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the derivative:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\frac{\\partial \\psi(\\theta)}{\\partial \\theta} \\\\\n",
    "=& \\frac{\\partial L(z(t_1))}{\\partial z(t_1)}\\frac{\\partial z(t_1)}{\\partial \\theta} \\\\\n",
    "&- \\lambda(t_1) \\frac{\\partial z(t_1)}{\\partial \\theta} + \\int_{t_0}^{t_1} (\\frac{d \\lambda(t)}{dt} \\frac{\\partial z(t)}{\\partial \\theta} + \\lambda(t) ( \\frac{\\partial f(z(t), t, \\theta)}{\\partial z}\\frac{\\partial z}{\\partial \\theta} + \\frac{\\partial f(z(t), t, \\theta)}{\\partial \\theta} ) ) dt \\\\\n",
    "=& (\\frac{\\partial L(z(t_1))}{\\partial z(t_1)} - \\lambda(t_1)) \\frac{\\partial z(t_1)}{\\partial \\theta} \\\\\n",
    "& + \\int_{t_0}^{t_1} (\\frac{d \\lambda(t)}{dt}  + \\lambda(t)\\frac{\\partial f(z(t), t, \\theta)}{\\partial z})\\frac{\\partial z}{\\partial \\theta} dt + \\int_{t_0}^{t_1} ( \\lambda(t) \\frac{\\partial f(z(t), t, \\theta)}{\\partial \\theta} )  dt \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this equation,\n",
    "- $\\frac{\\partial L(z(t_1))}{\\partial z(t_1)}$ is the gradient of loss with respect to $z(t_1)$. This is easy to compute.\n",
    "\n",
    "- $\\frac{\\partial z(t_1)}{\\partial \\theta}$ is the gradient of $z(t_1)$ with respect to the parameters. This is the difficult part.\n",
    "\n",
    "- $\\frac{\\partial f(z(t), t, \\theta)}{\\partial z}$ is the Jacobian matrix of the function $f(z(t), t, \\theta)$ with respect to $z(t)$. This is easy to compute.\n",
    "\n",
    "- $\\frac{\\partial z(t)}{\\partial \\theta}$ is the gradient of $z(t)$ with respect to the parameters. This is the difficult part.\n",
    "\n",
    "- $\\frac{\\partial f(z(t), t, \\theta)}{\\partial \\theta}$ is the Jacobian matrix of the function $f(z(t), t, \\theta)$ with respect to $\\theta$. This is easy to compute.\n",
    "\n",
    "Now we want to choose appropriate $\\lambda(t)$ such that the difficult part is not needed. We want to choose $\\lambda(t)$ such that $\\frac{\\partial \\psi(\\theta)}{\\partial \\theta}$ is easy to compute.\n",
    "$$\n",
    "\\frac{d \\lambda}{dt} = -\\lambda(t) \\frac{\\partial f(z(t), t, \\theta)}{\\partial z}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\lambda(t_1) = \\frac{\\partial L(z(t_1))}{\\partial z(t_1)}\n",
    "$$\n",
    "Then the gradient will simplify to\n",
    "$$\n",
    "\\frac{\\partial \\psi(\\theta)}{\\partial \\theta} = \\int_{t_0}^{t_1} ( \\lambda(t) \\frac{\\partial f(z(t), t, \\theta)}{\\partial \\theta} )  dt\n",
    "$$\n",
    "\n",
    "In summary, the function $\\lambda(t)$ should satisfy the following ODE:\n",
    "$$\n",
    "\\frac{d \\lambda}{dt} = -\\lambda(t) \\frac{\\partial f(z(t), t, \\theta)}{\\partial z}\n",
    "$$\n",
    "with the terminal condition at $t_1$:\n",
    "$$\n",
    "\\lambda(t_1) = \\frac{\\partial L(z(t_1))}{\\partial z(t_1)}\n",
    "$$\n",
    "The gradient is\n",
    "$$\n",
    "\\frac{\\partial L(z(t_1))}{\\partial \\theta} = - \\int_{t_1}^{t_0}  \\lambda(t) \\frac{\\partial f(z(t), t, \\theta)}{\\partial \\theta}   dt\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm summary:\n",
    "\n",
    "1. Forward: solve the ODE for $z(t)$ from $t_0$ to $t_1$ and get the output $z(t_1)$.\n",
    "\n",
    "2. Loss calculation: compute the loss $L(z(t_1))$.\n",
    "\n",
    "3. Backward: solve the ODE for $\\lambda(t)$ from $t_1$ to $t_0$ and get the gradient $\\frac{\\partial L(z(t_1))}{\\partial \\theta}$.\n",
    "\n",
    "4. Use the gradient and optimization algorithm to update the parameters $\\theta$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
