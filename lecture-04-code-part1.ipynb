{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ce48782",
   "metadata": {},
   "source": [
    "## The topic of this lecture:\n",
    "1. how to use available dataset in PyTorch\n",
    "2. how to use Feedforward Neural Network to do image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408d5dcc",
   "metadata": {},
   "source": [
    "### Step 1: Loading MNIST Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28c140c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "# MNIST dataset\n",
    "# https://en.wikipedia.org/wiki/MNIST_database\n",
    "\n",
    "# download and save training data and testing data\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39b9231a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ba640d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4b615c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x246ccef0c10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img, label = train_dataset[2]\n",
    "print(img.shape)\n",
    "print(label)\n",
    "\n",
    "plt.figure()\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8572b561",
   "metadata": {},
   "source": [
    "### Step 2: Make Dataset Iterable\n",
    "\n",
    "#### Batch sizes and iterations\n",
    "\n",
    "Because we have 60000 training samples (images), we need to split them up to small groups (batches) and pass these batches of samples to our feedforward neural network subsesquently.\n",
    "\n",
    "There are a few reasons why we split them into batches. Passing your whole dataset as a single batch would:\n",
    "\n",
    "(1) require a lot of RAM/VRAM on your CPU/GPU and this might result in Out-of-Memory (OOM) errors.\n",
    "\n",
    "(2) cause unstable training if you just use all the errors accumulated in 60,000 images to update the model rather than gradually update the model. In layman terms, imagine you accumulated errors for a student taking an exam with 60,000 questions and punish the student all at the same time. It is much harder for the student to learn compared to letting the student learn it made mistakes and did well in smaller batches of questions like mini-tests!\n",
    "\n",
    "If we have 60,000 images and we want a batch size of 100, then we would have 600 iterations where each iteration involves passing 600 images to the model and getting their respective predictions.\n",
    "\n",
    "#### Epoch\n",
    "\n",
    "An epoch means that you have successfully passed the whole training set, 60,000 images, to the model. Continuing our example above, an epoch consists of 600 iterations.\n",
    "\n",
    "If we want to go through the whole dataset 5 times (5 epochs) for the model to learn, then we need 3000 iterations (600 x 5).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7611c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "n_iters = 6000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "print(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb43729c",
   "metadata": {},
   "source": [
    "### Step 3: Create Model Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39c62bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        \n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "\n",
    "        # Non-linearity\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Linear function  # LINEAR\n",
    "        out = self.fc1(x)\n",
    "\n",
    "        # Non-linearity  # NON-LINEAR\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        # Linear function (readout)  # LINEAR\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf51c9d4",
   "metadata": {},
   "source": [
    "### Step 4: Instantiate Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4df0410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c6c13",
   "metadata": {},
   "source": [
    "### Step 5: Instantiate Loss Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504d04d1",
   "metadata": {},
   "source": [
    "#### Cross entropy\n",
    "The cross entropy measures the performance of a classification model whose output is a probability value between 0 and 1. For two probability distributions $p=(p_1,p_2,\\cdots,p_n)$ and $q=(q_1,q_2,\\cdots,q_n)$ is defined as:\n",
    "$$l(p,q) = - \\sum_{i=1}^n q_i \\log p_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f68f1b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47965000629754095\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "q = np.array([0, 1, 0])             # True probability (from data)\n",
    "p = np.array([0.228, 0.619, 0.153]) # Predicted probability (from the output of neural network)\n",
    "\n",
    "print(-np.sum(q * np.log(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16b441af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0020020026706730793\n"
     ]
    }
   ],
   "source": [
    "# perfect prediction\n",
    "p = np.array([0.001, 0.998, 0.001])\n",
    "print(-np.sum(q * np.log(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2eff7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.907755278982137\n"
     ]
    }
   ],
   "source": [
    "# bad prediction\n",
    "p = np.array([0.001, 0.001, 0.998])\n",
    "print(-np.sum(q * np.log(p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e81c6b",
   "metadata": {},
   "source": [
    "#### Softmax function\n",
    "Softmax function: $\\sigma: \\mathbb{R}^n \\rightarrow (0,1)^n$ is defined:\n",
    "$$\n",
    "\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}},\n",
    "$$\n",
    "for $i=1,2,\\cdots,n$ and $z=(z_1,z_2,\\cdots,z_n)\\in\\mathbb{R}^n$.\n",
    "\n",
    "In simple words, it applies the standard exponential function to each element $z_{i}$ of the input vector and normalizes these values by dividing by the sum of all these exponentials. This normalization ensures that the sum of the components of the output vector is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "caf53f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.])\n",
      "tensor([0.2689, 0.7311])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax(dim=0)\n",
    "input = torch.tensor([2.,3.])\n",
    "print(input)\n",
    "\n",
    "output = m(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a20d5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8a18d2",
   "metadata": {},
   "source": [
    "### Step 6: Instantiate Optimizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ef5b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa53682",
   "metadata": {},
   "source": [
    "### Step 7: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ebe00cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.6232898831367493. Accuracy: 85.48999786376953%\n",
      "Iteration: 1000. Loss: 0.36981120705604553. Accuracy: 89.23999786376953%\n",
      "Iteration: 1500. Loss: 0.44988223910331726. Accuracy: 90.56999969482422%\n",
      "Iteration: 2000. Loss: 0.29529625177383423. Accuracy: 91.13999938964844%\n",
      "Iteration: 2500. Loss: 0.3359542191028595. Accuracy: 91.58999633789062%\n",
      "Iteration: 3000. Loss: 0.16718728840351105. Accuracy: 92.05000305175781%\n",
      "Iteration: 3500. Loss: 0.22497308254241943. Accuracy: 92.33999633789062%\n",
      "Iteration: 4000. Loss: 0.319786012172699. Accuracy: 92.58999633789062%\n",
      "Iteration: 4500. Loss: 0.1389559656381607. Accuracy: 92.80000305175781%\n",
      "Iteration: 5000. Loss: 0.13239392638206482. Accuracy: 93.0999984741211%\n",
      "Iteration: 5500. Loss: 0.1964728981256485. Accuracy: 93.27999877929688%\n",
      "Iteration: 6000. Loss: 0.18414334952831268. Accuracy: 93.48999786376953%\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            \n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                \n",
    "                # Load images with gradient accumulation capabilities\n",
    "                images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}%'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f407fff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
