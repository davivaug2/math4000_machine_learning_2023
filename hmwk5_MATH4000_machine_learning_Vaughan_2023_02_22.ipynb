{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e26f65",
   "metadata": {},
   "source": [
    "#  David Vaughan  R1166390\n",
    "# (MATH-4000-004)\n",
    "# Homework 5 02/22/2023\n",
    "#  Spring 2023 TTU Selected Topics: Machine learning model order reduction for differential equations (MATH-4000-004) Full Term: Homework for week 5/6 (due 02/22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e09a1d8",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e451911",
   "metadata": {},
   "source": [
    "Consider the matrix $A$:\n",
    "$$\n",
    "A =\n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 2 & 3 & 4 \\\\\n",
    "1 & 2 & 3 & 4 & 0 \\\\\n",
    "2 & 3 & 4 & 0 & 1 \\\\\n",
    "3 & 4 & 0 & 1 & 2 \\\\\n",
    "4 & 0 & 1 & 2 & 3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and the convolutional layer Conv2d with the property (in_channels=1, out_channels=1, kernel_size=2, stride=1, padding=1) and the kernel $K$\n",
    "$$\n",
    "K =\n",
    "\\begin{pmatrix}\n",
    "0 & 1 \\\\\n",
    "1 & 2 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Compute the output Conv2d(A) using Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "de40deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "12bf2dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape torch.Size([1, 5, 5])\n",
      "tensor([[[0., 1., 2., 3., 4.],\n",
      "         [1., 2., 3., 4., 0.],\n",
      "         [2., 3., 4., 0., 1.],\n",
      "         [3., 4., 0., 1., 2.],\n",
      "         [4., 0., 1., 2., 3.]]])\n"
     ]
    }
   ],
   "source": [
    "#could transpose\n",
    "#maybe way to use for loop or something to make this?\n",
    "A =torch.tensor([([[ 0.,1.,2.,3.,4.],\n",
    "        [ 1.,2.,3.,4.,0.],\n",
    "        [ 2.,3.,4.,0.,1.],\n",
    "        [ 3.,4.,0.,1.,2.],\n",
    "        [ 4.,0.,1.,2.,3.]])])\n",
    "#w =torch.tensor([[ 0,1],\n",
    " #       [ 1,2]])\n",
    "\n",
    "\n",
    "print(\"shape\",A.shape)\n",
    "print(A)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "93adb84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : \n",
      " tensor([[[[0.1787, 0.4544],\n",
      "          [0.0884, 0.4732]]]])\n",
      "bias : \n",
      " tensor([0.3287])\n",
      "weight : \n",
      " tensor([[[[0., 1.],\n",
      "          [1., 2.]]]])\n",
      "bias : \n",
      " tensor([0.3287])\n"
     ]
    }
   ],
   "source": [
    "cnn = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2,stride=1,padding=1)\n",
    "# weight: of size (out_channels, in_channels, kernel_size[0], kernel_size[1])\n",
    "# bias: of size (out_channels)\n",
    "for name, param in cnn.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, ': \\n', param.data)\n",
    "w = cnn.weight\n",
    "w_np = w.detach().numpy()\n",
    "w_np[0,0,0,:] = 0.,1.\n",
    "w_np[0,0,1,:] = 1.,2\n",
    "#w_np[0,0] = 0,1\n",
    "#w_np[0,0] = 1,2\n",
    "cnn.weight = torch.nn.Parameter(torch.from_numpy(w_np))\n",
    "for name, param in cnn.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, ': \\n', param.data) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1a9340af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output matrix: \n",
      " tensor([[[ 0.3287,  2.3287,  5.3287,  8.3287, 11.3287,  4.3287],\n",
      "         [ 2.3287,  6.3287, 10.3287, 14.3287,  8.3287,  0.3287],\n",
      "         [ 5.3287, 10.3287, 14.3287,  8.3287,  2.3287,  1.3287],\n",
      "         [ 8.3287, 14.3287,  8.3287,  2.3287,  6.3287,  2.3287],\n",
      "         [11.3287,  8.3287,  2.3287,  6.3287, 10.3287,  3.3287],\n",
      "         [ 4.3287,  0.3287,  1.3287,  2.3287,  3.3287,  0.3287]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "out = cnn(A)\n",
    "print('output matrix: \\n', out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9865077c",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "For the same matrix $A$ in Problem 1, consider the Max Pooling layer MaxPool2d with the property (kernel_size=2, stride=1, padding=1). Compute the output MaxPool2d(A) using Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bd897f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A is : \n",
      " tensor([[[0., 1., 2., 3., 4.],\n",
      "         [1., 2., 3., 4., 0.],\n",
      "         [2., 3., 4., 0., 1.],\n",
      "         [3., 4., 0., 1., 2.],\n",
      "         [4., 0., 1., 2., 3.]]])\n",
      "output image: \n",
      " tensor([[[0., 1., 2., 3., 4., 4.],\n",
      "         [1., 2., 3., 4., 4., 4.],\n",
      "         [2., 3., 4., 4., 4., 1.],\n",
      "         [3., 4., 4., 4., 2., 2.],\n",
      "         [4., 4., 4., 2., 3., 3.],\n",
      "         [4., 4., 1., 2., 3., 3.]]])\n"
     ]
    }
   ],
   "source": [
    "maxpool2 = nn.MaxPool2d(kernel_size=2, stride=1, padding=1)\n",
    "out2 = maxpool2(A)\n",
    "print('A is : \\n', A)\n",
    "print('output image: \\n', out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8f0358",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "#### (this problem is optional and will not be counted in grade)\n",
    "For two probability distributions $p=(p_1,p_2,\\cdots,p_n)$ and $q=(q_1,q_2,\\cdots,q_n)$, the cross entropy is defined as: $$l(p,q) = - \\sum_{i=1}^n q_i \\log p_i$$\n",
    "Prove that\n",
    "$$ l(p,q) \\ge l(q,q)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924dad73",
   "metadata": {},
   "source": [
    "# Proveing things is for mathmaticians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df93c52",
   "metadata": {},
   "source": [
    "Code for me to look at in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2068b092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [1, 2]])\n",
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [1, 2, 3, 4, 0],\n",
      "        [2, 3, 4, 0, 1],\n",
      "        [3, 4, 0, 1, 2],\n",
      "        [4, 0, 1, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "#could transpose\n",
    "#maybe way to use for loop or something to make this?\n",
    "A =torch.tensor([[ 0,1,2,3,4],\n",
    "        [ 1,2,3,4,0],\n",
    "        [ 2,3,4,0,1],\n",
    "        [ 3,4,0,1,2],\n",
    "        [ 4,0,1,2,3]])\n",
    "w =torch.tensor([[ 0,1],\n",
    "        [ 1,2]])\n",
    " \n",
    "print(w)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76933068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : \n",
      " tensor([[[[-0.2018,  0.1330],\n",
      "          [ 0.1719,  0.3067]]]])\n",
      "bias : \n",
      " tensor([0.4281])\n",
      "weight : \n",
      " tensor([[[[0., 1.],\n",
      "          [1., 2.]]]])\n",
      "bias : \n",
      " tensor([0.4281])\n"
     ]
    }
   ],
   "source": [
    "cnn = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2,stride=1,padding=1)\n",
    "# weight: of size (out_channels, in_channels, kernel_size[0], kernel_size[1])\n",
    "# bias: of size (out_channels)\n",
    "for name, param in cnn.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, ': \\n', param.data)\n",
    "w = cnn.weight\n",
    "w_np = w.detach().numpy()\n",
    "w_np[0,0,0,:] = 0,1\n",
    "w_np[0,0,1,:] = 1,2\n",
    "#w_np[0,0] = 0,1\n",
    "#w_np[0,0] = 1,2\n",
    "cnn.weight = torch.nn.Parameter(torch.from_numpy(w_np))\n",
    "for name, param in cnn.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, ': \\n', param.data) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb2a89c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (__int64) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17140\\3838559290.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output matrix: \\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\kagleML\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\kagleML\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\kagleML\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 459\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (__int64) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "out = cnn(A)\n",
    "print('output matrix: \\n', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f1be5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
