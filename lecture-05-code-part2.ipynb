{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network\n",
    "\n",
    "- Usage: analyze sequential data, for example a sequence of images in a video, language translation\n",
    "\n",
    "- What is sequential data (example: predict the next word):\n",
    "\n",
    "    - I went to have dinner with my .... (possible answer: friend/parents/roommate)\n",
    "\n",
    "    - My dad and mom came to campus for my graduation commencement two days ago. I went to have dinner with my .... (possible answer: parents)\n",
    "\n",
    "- Features of RNN:\n",
    "    - variable-length sequences\n",
    "    - track long-term dependencies\n",
    "    - maintain information about the order\n",
    "    - share parameters across the sequence (reduce the number of parameters in the RNN; the number of parameters should be independent of the length of the seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical formulation of recurrent layer:\n",
    "$$\n",
    "h_t = \\sigma(W_{ih} x_t + b_{ih} + W_{hh} h_{t-1} + b_{hh})\n",
    "$$\n",
    "\n",
    "- $h_t$: hidden state at time $t$\n",
    "- $x_t$: input at time $t$\n",
    "- $W_{ih}$: input-hidden weights\n",
    "- $W_{hh}$: hidden-hidden weights\n",
    "- $b_{ih}$: input-hidden bias\n",
    "- $b_{hh}$: hidden-hidden bias\n",
    "- $\\sigma$: activation function (ReLU or tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: one recurrent layer\n",
    "- input (example: a sentence \"I like machine learning\", x1 = \"I\", x2 = \"like\", x3 = \"learning\"\n",
    "$$\n",
    "input = (x_1, x_2, x_3, x_4)\n",
    "$$\n",
    "- one recurrent layer:\n",
    "$$\n",
    "h_1 = \\sigma(W_{ih} x_1 + b_{ih} + W_{hh} h_{0} + b_{hh})\n",
    "$$\n",
    "$$\n",
    "h_2 = \\sigma(W_{ih} x_2 + b_{ih} + W_{hh} h_{1} + b_{hh})\n",
    "$$\n",
    "$$\n",
    "h_3 = \\sigma(W_{ih} x_3 + b_{ih} + W_{hh} h_{2} + b_{hh})\n",
    "$$\n",
    "$$\n",
    "h_4 = \\sigma(W_{ih} x_4 + b_{ih} + W_{hh} h_{3} + b_{hh})\n",
    "$$\n",
    "- output:\n",
    "$$\n",
    "output = (h_1, h_2, h_3, h_4)\n",
    "$$\n",
    "#### Note\n",
    "- The initial hidden state $h_0$ is usually taken as either zero or random tensor\n",
    "- share parameters ($W_{ih}$, $W_{hh}$, $b_{ih}$, $b_{ih}$) across the sequence\n",
    "- input can be arbitrary length: $(x_1, x_2, \\cdots, x_n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 50]) torch.Size([1, 32, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# input_size: The number of expected features in the input x\n",
    "# hidden_size: The number of features in the hidden state h\n",
    "# num_layers: Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1\n",
    "rnn = torch.nn.RNN(input_size=20, hidden_size=50)\n",
    "\n",
    "# input: (sequence_length, batch_size/sample_number, input_size)\n",
    "input = torch.randn(3, 32, 20)\n",
    "x1, x2, x3 = input[0], input[1], input[2]\n",
    "\n",
    "# h_0, initial hidden state: (num_layers, batch_size, hidden_size)\n",
    "h_0 = torch.zeros(1, 32 ,50)\n",
    "\n",
    "# output: (sequence_length, batch_size, hidden_size)\n",
    "# h_n, final hidden state: (num_layers, batch_size, hidden_size)\n",
    "output, hn = rnn(input, h_0)\n",
    "\n",
    "print(output.size(), hn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: two recurrent layers\n",
    "- input:\n",
    "$$\n",
    "input = (x_1, x_2, x_3)\n",
    "$$\n",
    "\n",
    "- layer 1:\n",
    "$$\n",
    "h_1^{[1]} = \\sigma(W_{ih}^{[1]} x_1 + b_{ih}^{[1]} + W_{hh}^{[1]} h_{0}^{[1]} + b_{hh}^{[1]})\n",
    "$$\n",
    "$$\n",
    "h_2^{[1]} = \\sigma(W_{ih}^{[1]} x_2 + b_{ih}^{[1]} + W_{hh}^{[1]} h_{1}^{[1]} + b_{hh}^{[1]})\n",
    "$$\n",
    "$$\n",
    "h_3^{[1]} = \\sigma(W_{ih}^{[1]} x_3 + b_{ih}^{[1]} + W_{hh}^{[1]} h_{2}^{[1]} + b_{hh}^{[1]})\n",
    "$$\n",
    "Note: share parameters ($W_{ih}^{[1]}$, $W_{hh}^{[1]}$, $b_{ih}^{[1]}$, $b_{ih}^{[1]}$) across the sequence\n",
    "\n",
    "- layer 2:\n",
    "$$\n",
    "h_1^{[2]} = \\sigma(W_{ih}^{[2]} h_1^{[1]} + b_{ih}^{[2]} + W_{hh}^{[2]} h_{0}^{[2]} + b_{hh}^{[2]})\n",
    "$$\n",
    "$$\n",
    "h_2^{[2]} = \\sigma(W_{ih}^{[2]} h_2^{[1]} + b_{ih}^{[2]} + W_{hh}^{[2]} h_{1}^{[2]} + b_{hh}^{[2]})\n",
    "$$\n",
    "$$\n",
    "h_3^{[2]} = \\sigma(W_{ih}^{[2]} h_3^{[1]} + b_{ih}^{[2]} + W_{hh}^{[2]} h_{2}^{[2]} + b_{hh}^{[2]})\n",
    "$$\n",
    "Note: share parameters ($W_{ih}^{[2]}$, $W_{hh}^{[2]}$, $b_{ih}^{[2]}$, $b_{ih}^{[2]}$) across the sequence\n",
    "\n",
    "- output:\n",
    "$$\n",
    "output = (h_1^{[2]}, h_2^{[2]}, h_3^{[2]})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alternative text](deep-rnn.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 50]) torch.Size([2, 32, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# input_size: The number of expected features in the input x\n",
    "# hidden_size: The number of features in the hidden state h\n",
    "# num_layers: Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1\n",
    "rnn = torch.nn.RNN(input_size=20, hidden_size=50, num_layers=2)\n",
    "\n",
    "# input: (sequence_length, batch_size, input_size)\n",
    "input = torch.randn(3, 32, 20)\n",
    "# x1, x2, x3 = input[0], input[1], input[2]\n",
    "\n",
    "# h_0, initial hidden state: (num_layers, batch_size, hidden_size)\n",
    "h_0 = torch.randn(2, 32 ,50)\n",
    "\n",
    "# output: (sequence_length, batch_size, hidden_size)\n",
    "# h_n, final hidden state: (num_layers, batch_size, hidden_size)\n",
    "output, hn = rnn(input, h_0)\n",
    "\n",
    "print(output.size(), hn.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "64165d061f1520ab7d121a7a19199cbac06e5deac1296a5016743a69ede8fd69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
